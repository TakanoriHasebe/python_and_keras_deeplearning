{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みのネットワークとは、大規模なデータセットで訓練されたあと、保存されたネットワークのこと。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、VGG16アーキテクチャを使用する。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みのネットワークを使用するには、以下の２つがある。<br>\n",
    "1. 特徴抽出<br>\n",
    "2. ファインチューニング<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像分類に使用されるCNNは２つの部分で構成されている。<br>\n",
    "CNNは、一連のプーリング層と畳み込み層で始まり、全結合分類器で終わる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(\n",
    "        weights = 'imagenet', # 重みのチェックポイント\n",
    "        include_top = False, # 全結合分類器を含めるかどうか\n",
    "        input_shape=(150,150,3)) # 引数を指定しない場合、ネットワークは任意のサイズを処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ拡張を行わない高速な特徴抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/Takanori/Downloads/cats_and_dogs_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255) # データの作成\n",
    "batch_size = 20 # バッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "[[[[0.6156863  0.57254905 0.454902  ]\n",
      "   [0.6117647  0.5686275  0.45098042]\n",
      "   [0.64705884 0.6039216  0.48627454]\n",
      "   ...\n",
      "   [0.7725491  0.81568635 0.8000001 ]\n",
      "   [0.7725491  0.81568635 0.8000001 ]\n",
      "   [0.7725491  0.81568635 0.8000001 ]]\n",
      "\n",
      "  [[0.63529414 0.5921569  0.47450984]\n",
      "   [0.6156863  0.57254905 0.454902  ]\n",
      "   [0.62352943 0.5803922  0.46274513]\n",
      "   ...\n",
      "   [0.7725491  0.81568635 0.8000001 ]\n",
      "   [0.7725491  0.81568635 0.8000001 ]\n",
      "   [0.7725491  0.81568635 0.8000001 ]]\n",
      "\n",
      "  [[0.61960787 0.5764706  0.45882356]\n",
      "   [0.63529414 0.5921569  0.47450984]\n",
      "   [0.6039216  0.56078434 0.4431373 ]\n",
      "   ...\n",
      "   [0.7725491  0.81568635 0.8000001 ]\n",
      "   [0.7725491  0.81568635 0.8000001 ]\n",
      "   [0.7725491  0.81568635 0.8000001 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.19607845 0.19607845 0.20392159]\n",
      "   [0.18823531 0.18823531 0.19607845]\n",
      "   [0.21568629 0.21176472 0.23137257]\n",
      "   ...\n",
      "   [0.21568629 0.16862746 0.21568629]\n",
      "   [0.21568629 0.16862746 0.21568629]\n",
      "   [0.21568629 0.16862746 0.21568629]]\n",
      "\n",
      "  [[0.1764706  0.1764706  0.18431373]\n",
      "   [0.16862746 0.16470589 0.18431373]\n",
      "   [0.19215688 0.18823531 0.20784315]\n",
      "   ...\n",
      "   [0.21960786 0.16470589 0.21568629]\n",
      "   [0.21568629 0.16862746 0.21568629]\n",
      "   [0.21568629 0.16862746 0.21568629]]\n",
      "\n",
      "  [[0.16078432 0.15686275 0.18823531]\n",
      "   [0.16078432 0.15686275 0.18823531]\n",
      "   [0.16078432 0.15686275 0.18823531]\n",
      "   ...\n",
      "   [0.19607845 0.16470589 0.20784315]\n",
      "   [0.19607845 0.16470589 0.20784315]\n",
      "   [0.20392159 0.16470589 0.20784315]]]\n",
      "\n",
      "\n",
      " [[[0.8117648  0.7803922  0.5372549 ]\n",
      "   [0.8117648  0.7803922  0.53333336]\n",
      "   [0.6901961  0.65882355 0.41176474]\n",
      "   ...\n",
      "   [0.7725491  0.81568635 0.90196085]\n",
      "   [0.80392164 0.8352942  0.9176471 ]\n",
      "   [0.8431373  0.86666673 0.95294124]]\n",
      "\n",
      "  [[0.7843138  0.76470596 0.49803925]\n",
      "   [0.73333335 0.7137255  0.44705886]\n",
      "   [0.8117648  0.79215693 0.5254902 ]\n",
      "   ...\n",
      "   [0.7960785  0.82745105 0.9176471 ]\n",
      "   [0.8196079  0.85098046 0.9333334 ]\n",
      "   [0.854902   0.882353   0.9568628 ]]\n",
      "\n",
      "  [[0.69803923 0.7019608  0.38823533]\n",
      "   [0.73333335 0.7372549  0.42352945]\n",
      "   [0.7725491  0.77647066 0.46274513]\n",
      "   ...\n",
      "   [0.86274517 0.8941177  0.97647065]\n",
      "   [0.87843144 0.9058824  0.9803922 ]\n",
      "   [0.8980393  0.9176471  0.9921569 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8313726  0.8352942  0.6117647 ]\n",
      "   [0.85098046 0.8588236  0.6117647 ]\n",
      "   [0.8235295  0.8352942  0.5568628 ]\n",
      "   ...\n",
      "   [0.8431373  0.85098046 0.5647059 ]\n",
      "   [0.93725497 0.9450981  0.65882355]\n",
      "   [0.7803922  0.78823537 0.5019608 ]]\n",
      "\n",
      "  [[0.8000001  0.81568635 0.61960787]\n",
      "   [0.8196079  0.85098046 0.61960787]\n",
      "   [0.80392164 0.85098046 0.5686275 ]\n",
      "   ...\n",
      "   [0.81568635 0.81568635 0.5647059 ]\n",
      "   [0.6509804  0.654902   0.3803922 ]\n",
      "   [0.62352943 0.6313726  0.34509805]]\n",
      "\n",
      "  [[0.79215693 0.8235295  0.6392157 ]\n",
      "   [0.81568635 0.8588236  0.6392157 ]\n",
      "   [0.64705884 0.7176471  0.427451  ]\n",
      "   ...\n",
      "   [0.909804   0.9058824  0.69411767]\n",
      "   [0.86666673 0.86666673 0.6156863 ]\n",
      "   [0.7254902  0.7294118  0.454902  ]]]\n",
      "\n",
      "\n",
      " [[[0.9921569  0.98823535 0.9803922 ]\n",
      "   [0.9803922  0.97647065 0.9686275 ]\n",
      "   [0.95294124 0.95294124 0.9607844 ]\n",
      "   ...\n",
      "   [0.94117653 0.92549026 0.9294118 ]\n",
      "   [0.9803922  0.9725491  0.97647065]\n",
      "   [0.9960785  0.98823535 0.9921569 ]]\n",
      "\n",
      "  [[0.9686275  0.96470594 0.9568628 ]\n",
      "   [0.909804   0.90196085 0.9058824 ]\n",
      "   [0.8352942  0.82745105 0.83921576]\n",
      "   ...\n",
      "   [0.8313726  0.8078432  0.81568635]\n",
      "   [0.9333334  0.9176471  0.9215687 ]\n",
      "   [0.9725491  0.96470594 0.9686275 ]]\n",
      "\n",
      "  [[0.95294124 0.93725497 0.94117653]\n",
      "   [0.86274517 0.8470589  0.85098046]\n",
      "   [0.7411765  0.72156864 0.74509805]\n",
      "   ...\n",
      "   [0.72156864 0.6784314  0.69411767]\n",
      "   [0.8862746  0.86274517 0.8705883 ]\n",
      "   [0.9686275  0.95294124 0.9568628 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.95294124 0.95294124 0.9607844 ]\n",
      "   [0.909804   0.8745099  0.87843144]\n",
      "   [0.8470589  0.7686275  0.7725491 ]\n",
      "   ...\n",
      "   [0.8078432  0.75294125 0.75294125]\n",
      "   [0.90196085 0.86666673 0.87843144]\n",
      "   [0.9490197  0.94117653 0.95294124]]\n",
      "\n",
      "  [[0.97647065 0.9607844  0.96470594]\n",
      "   [0.93725497 0.9215687  0.92549026]\n",
      "   [0.89019614 0.8745099  0.87843144]\n",
      "   ...\n",
      "   [0.87843144 0.87843144 0.87843144]\n",
      "   [0.9176471  0.9176471  0.9176471 ]\n",
      "   [0.9607844  0.9607844  0.9607844 ]]\n",
      "\n",
      "  [[1.         0.98823535 0.9921569 ]\n",
      "   [0.98823535 0.9725491  0.97647065]\n",
      "   [0.9568628  0.94117653 0.9450981 ]\n",
      "   ...\n",
      "   [0.9568628  0.9568628  0.9568628 ]\n",
      "   [0.97647065 0.97647065 0.97647065]\n",
      "   [0.9921569  0.9921569  0.9921569 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.63529414 0.3647059  0.3372549 ]\n",
      "   [0.6784314  0.3921569  0.3647059 ]\n",
      "   [0.72156864 0.41960788 0.3803922 ]\n",
      "   ...\n",
      "   [0.40784317 0.37647063 0.29411766]\n",
      "   [0.41176474 0.36078432 0.28627452]\n",
      "   [0.41176474 0.36078432 0.28627452]]\n",
      "\n",
      "  [[0.3529412  0.16470589 0.14901961]\n",
      "   [0.42352945 0.22352943 0.21176472]\n",
      "   [0.5176471  0.3019608  0.28235295]\n",
      "   ...\n",
      "   [0.40784317 0.37647063 0.29411766]\n",
      "   [0.40784317 0.35686275 0.28235295]\n",
      "   [0.40784317 0.35686275 0.28235295]]\n",
      "\n",
      "  [[0.34901962 0.2901961  0.31764707]\n",
      "   [0.3372549  0.27058825 0.29411766]\n",
      "   [0.32156864 0.24313727 0.2509804 ]\n",
      "   ...\n",
      "   [0.40784317 0.37647063 0.3019608 ]\n",
      "   [0.42352945 0.37254903 0.30588236]\n",
      "   [0.42352945 0.37254903 0.30588236]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.49803925 0.4666667  0.38431376]\n",
      "   [0.50980395 0.4784314  0.39607847]\n",
      "   [0.52156866 0.4901961  0.40784317]\n",
      "   ...\n",
      "   [0.5647059  0.5411765  0.4784314 ]\n",
      "   [0.59607846 0.57254905 0.50980395]\n",
      "   [0.59607846 0.57254905 0.50980395]]\n",
      "\n",
      "  [[0.5137255  0.48235297 0.40000004]\n",
      "   [0.5254902  0.49411768 0.41176474]\n",
      "   [0.5372549  0.5058824  0.42352945]\n",
      "   ...\n",
      "   [0.6039216  0.5803922  0.5176471 ]\n",
      "   [0.5647059  0.5411765  0.4784314 ]\n",
      "   [0.5647059  0.5411765  0.4784314 ]]\n",
      "\n",
      "  [[0.5176471  0.48627454 0.4039216 ]\n",
      "   [0.50980395 0.4784314  0.39607847]\n",
      "   [0.5058824  0.47450984 0.3921569 ]\n",
      "   ...\n",
      "   [0.6039216  0.5803922  0.5176471 ]\n",
      "   [0.5803922  0.5568628  0.49411768]\n",
      "   [0.5803922  0.5568628  0.49411768]]]\n",
      "\n",
      "\n",
      " [[[0.13725491 0.10196079 0.10588236]\n",
      "   [0.14117648 0.10588236 0.10980393]\n",
      "   [0.15294118 0.11764707 0.12156864]\n",
      "   ...\n",
      "   [0.17254902 0.13725491 0.14117648]\n",
      "   [0.16078432 0.1254902  0.12941177]\n",
      "   [0.13725491 0.10196079 0.10588236]]\n",
      "\n",
      "  [[0.14117648 0.10588236 0.10980393]\n",
      "   [0.14509805 0.10980393 0.1137255 ]\n",
      "   [0.15686275 0.12156864 0.1254902 ]\n",
      "   ...\n",
      "   [0.19607845 0.16078432 0.16470589]\n",
      "   [0.1764706  0.14117648 0.14509805]\n",
      "   [0.14901961 0.1137255  0.11764707]]\n",
      "\n",
      "  [[0.14117648 0.10588236 0.10980393]\n",
      "   [0.15294118 0.11764707 0.12156864]\n",
      "   [0.16862746 0.13333334 0.13725491]\n",
      "   ...\n",
      "   [0.21960786 0.18431373 0.18823531]\n",
      "   [0.18823531 0.15294118 0.15686275]\n",
      "   [0.16078432 0.1254902  0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.14509805 0.10980393 0.1137255 ]\n",
      "   [0.14901961 0.1137255  0.11764707]\n",
      "   [0.16470589 0.12941177 0.13333334]\n",
      "   ...\n",
      "   [0.20000002 0.16470589 0.16862746]\n",
      "   [0.18823531 0.15294118 0.15686275]\n",
      "   [0.16862746 0.13333334 0.13725491]]\n",
      "\n",
      "  [[0.13333334 0.09803922 0.10196079]\n",
      "   [0.14901961 0.1137255  0.11764707]\n",
      "   [0.16078432 0.1254902  0.12941177]\n",
      "   ...\n",
      "   [0.18431373 0.14901961 0.15294118]\n",
      "   [0.17254902 0.13725491 0.14117648]\n",
      "   [0.14901961 0.1137255  0.11764707]]\n",
      "\n",
      "  [[0.13333334 0.09803922 0.10196079]\n",
      "   [0.14509805 0.10980393 0.1137255 ]\n",
      "   [0.15294118 0.11764707 0.12156864]\n",
      "   ...\n",
      "   [0.16470589 0.12941177 0.13333334]\n",
      "   [0.15294118 0.11764707 0.12156864]\n",
      "   [0.13725491 0.10196079 0.10588236]]]\n",
      "\n",
      "\n",
      " [[[0.2901961  0.23529413 0.18431373]\n",
      "   [0.37254903 0.32156864 0.25882354]\n",
      "   [0.3803922  0.32941177 0.25490198]\n",
      "   ...\n",
      "   [0.38431376 0.31764707 0.25490198]\n",
      "   [0.43529415 0.3803922  0.32941177]\n",
      "   [0.427451   0.3647059  0.23137257]]\n",
      "\n",
      "  [[0.6156863  0.5529412  0.49411768]\n",
      "   [0.5764706  0.5137255  0.45098042]\n",
      "   [0.44705886 0.38823533 0.3137255 ]\n",
      "   ...\n",
      "   [0.34901962 0.28235295 0.21176472]\n",
      "   [0.32156864 0.25882354 0.20000002]\n",
      "   [0.37647063 0.30588236 0.21176472]]\n",
      "\n",
      "  [[0.50980395 0.45098042 0.37647063]\n",
      "   [0.58431375 0.5254902  0.4431373 ]\n",
      "   [0.41176474 0.3529412  0.27058825]\n",
      "   ...\n",
      "   [0.34117648 0.27058825 0.1764706 ]\n",
      "   [0.33333334 0.27058825 0.18039216]\n",
      "   [0.38823533 0.30588236 0.22352943]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.54901963 0.5019608  0.34509805]\n",
      "   [0.3529412  0.29803923 0.19215688]\n",
      "   [0.35686275 0.30980393 0.15294118]\n",
      "   ...\n",
      "   [0.5372549  0.49803925 0.36078432]\n",
      "   [0.40000004 0.3372549  0.20784315]\n",
      "   [0.45882356 0.38431376 0.25490198]]\n",
      "\n",
      "  [[0.6039216  0.5568628  0.40000004]\n",
      "   [0.43529415 0.3803922  0.27450982]\n",
      "   [0.3254902  0.2784314  0.12156864]\n",
      "   ...\n",
      "   [0.36078432 0.31764707 0.20000002]\n",
      "   [0.4431373  0.37647063 0.26666668]\n",
      "   [0.36078432 0.3019608  0.18823531]]\n",
      "\n",
      "  [[0.62352943 0.5764706  0.41960788]\n",
      "   [0.40000004 0.34509805 0.2392157 ]\n",
      "   [0.6156863  0.5686275  0.41176474]\n",
      "   ...\n",
      "   [0.53333336 0.49411768 0.38823533]\n",
      "   [0.36862746 0.30588236 0.20784315]\n",
      "   [0.39607847 0.35686275 0.25882354]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.31764707 0.35686275 0.36078432]\n",
      "   [0.3254902  0.3647059  0.36862746]\n",
      "   [0.3372549  0.37647063 0.3803922 ]\n",
      "   ...\n",
      "   [0.5372549  0.61960787 0.6313726 ]\n",
      "   [0.53333336 0.6156863  0.627451  ]\n",
      "   [0.5254902  0.60784316 0.61960787]]\n",
      "\n",
      "  [[0.3019608  0.34901962 0.34901962]\n",
      "   [0.30588236 0.3529412  0.3529412 ]\n",
      "   [0.31764707 0.3647059  0.3647059 ]\n",
      "   ...\n",
      "   [0.5372549  0.61960787 0.6313726 ]\n",
      "   [0.5372549  0.61960787 0.6313726 ]\n",
      "   [0.5294118  0.6117647  0.62352943]]\n",
      "\n",
      "  [[0.3019608  0.34901962 0.34901962]\n",
      "   [0.3019608  0.34901962 0.34901962]\n",
      "   [0.3137255  0.36078432 0.36078432]\n",
      "   ...\n",
      "   [0.54901963 0.6313726  0.6431373 ]\n",
      "   [0.54901963 0.6313726  0.6431373 ]\n",
      "   [0.5411765  0.62352943 0.63529414]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.18431373 0.27058825 0.4156863 ]\n",
      "   [0.17254902 0.2627451  0.42352945]\n",
      "   [0.16078432 0.25882354 0.41960788]\n",
      "   ...\n",
      "   [0.35686275 0.46274513 0.6392157 ]\n",
      "   [0.32941177 0.454902   0.6039216 ]\n",
      "   [0.29411766 0.43529415 0.56078434]]\n",
      "\n",
      "  [[0.18039216 0.26666668 0.41176474]\n",
      "   [0.1764706  0.26666668 0.427451  ]\n",
      "   [0.16862746 0.26666668 0.427451  ]\n",
      "   ...\n",
      "   [0.3647059  0.47450984 0.65882355]\n",
      "   [0.3254902  0.45098042 0.6039216 ]\n",
      "   [0.28235295 0.42352945 0.5568628 ]]\n",
      "\n",
      "  [[0.21176472 0.29803923 0.4431373 ]\n",
      "   [0.21568629 0.30588236 0.4666667 ]\n",
      "   [0.20000002 0.29803923 0.45882356]\n",
      "   ...\n",
      "   [0.37254903 0.48235297 0.6784314 ]\n",
      "   [0.31764707 0.44705886 0.6156863 ]\n",
      "   [0.27058825 0.41176474 0.5529412 ]]]\n",
      "\n",
      "\n",
      " [[[0.18823531 0.1764706  0.21176472]\n",
      "   [0.18823531 0.1764706  0.21176472]\n",
      "   [0.18823531 0.1764706  0.21176472]\n",
      "   ...\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]]\n",
      "\n",
      "  [[0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   ...\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]]\n",
      "\n",
      "  [[0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   ...\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]\n",
      "   [0.18431373 0.17254902 0.20784315]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.50980395 0.4431373  0.47450984]\n",
      "   [0.4784314  0.40000004 0.43529415]\n",
      "   [0.5411765  0.45882356 0.49411768]\n",
      "   ...\n",
      "   [0.18823531 0.18039216 0.20000002]\n",
      "   [0.18431373 0.17254902 0.20000002]\n",
      "   [0.18431373 0.17254902 0.20784315]]\n",
      "\n",
      "  [[0.5176471  0.4666667  0.49411768]\n",
      "   [0.54901963 0.4901961  0.5176471 ]\n",
      "   [0.54509807 0.4666667  0.5019608 ]\n",
      "   ...\n",
      "   [0.18823531 0.18039216 0.20000002]\n",
      "   [0.18431373 0.17254902 0.20000002]\n",
      "   [0.18431373 0.17254902 0.20784315]]\n",
      "\n",
      "  [[0.5568628  0.4901961  0.5294118 ]\n",
      "   [0.5686275  0.4901961  0.5254902 ]\n",
      "   [0.5921569  0.50980395 0.5372549 ]\n",
      "   ...\n",
      "   [0.18823531 0.18039216 0.20000002]\n",
      "   [0.18431373 0.17254902 0.20000002]\n",
      "   [0.18431373 0.17254902 0.20784315]]]\n",
      "\n",
      "\n",
      " [[[0.24313727 0.20000002 0.07450981]\n",
      "   [0.32941177 0.30980393 0.14509805]\n",
      "   [0.16862746 0.15686275 0.03529412]\n",
      "   ...\n",
      "   [0.07843138 0.07843138 0.07843138]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.07843138 0.07843138 0.07843138]]\n",
      "\n",
      "  [[0.23137257 0.18823531 0.0627451 ]\n",
      "   [0.32156864 0.3019608  0.14509805]\n",
      "   [0.17254902 0.15686275 0.04313726]\n",
      "   ...\n",
      "   [0.06666667 0.06666667 0.06666667]\n",
      "   [0.05490196 0.05490196 0.05490196]\n",
      "   [0.07058824 0.07058824 0.07058824]]\n",
      "\n",
      "  [[0.23137257 0.18823531 0.07843138]\n",
      "   [0.32156864 0.29803923 0.15686275]\n",
      "   [0.1764706  0.16078432 0.05490196]\n",
      "   ...\n",
      "   [0.06666667 0.06666667 0.06666667]\n",
      "   [0.05490196 0.05490196 0.05490196]\n",
      "   [0.08235294 0.08235294 0.08235294]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.76470596 0.8196079  0.8705883 ]\n",
      "   [0.76470596 0.8196079  0.9333334 ]\n",
      "   [0.7411765  0.81568635 0.8000001 ]\n",
      "   ...\n",
      "   [0.58431375 0.47450984 0.32156864]\n",
      "   [0.5176471  0.46274513 0.28627452]\n",
      "   [0.5647059  0.4784314  0.31764707]]\n",
      "\n",
      "  [[0.72156864 0.7686275  0.8313726 ]\n",
      "   [0.6862745  0.7607844  0.7843138 ]\n",
      "   [0.6784314  0.74509805 0.86274517]\n",
      "   ...\n",
      "   [0.5568628  0.42352945 0.28627452]\n",
      "   [0.54509807 0.43137258 0.27450982]\n",
      "   [0.56078434 0.4666667  0.31764707]]\n",
      "\n",
      "  [[0.65882355 0.69803923 0.74509805]\n",
      "   [0.6627451  0.6666667  0.7372549 ]\n",
      "   [0.6509804  0.68235296 0.69411767]\n",
      "   ...\n",
      "   [0.57254905 0.4431373  0.30588236]\n",
      "   [0.56078434 0.43137258 0.3019608 ]\n",
      "   [0.58431375 0.454902   0.33333334]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.5019608  0.23529413 0.09019608]\n",
      "   [0.5137255  0.24705884 0.10196079]\n",
      "   [0.52156866 0.2509804  0.09803922]\n",
      "   ...\n",
      "   [0.6784314  0.43921572 0.19215688]\n",
      "   [0.69411767 0.454902   0.20784315]\n",
      "   [0.72156864 0.48235297 0.23529413]]\n",
      "\n",
      "  [[0.5019608  0.23529413 0.09019608]\n",
      "   [0.5137255  0.24705884 0.09411766]\n",
      "   [0.52156866 0.2509804  0.09803922]\n",
      "   ...\n",
      "   [0.6862745  0.44705886 0.20000002]\n",
      "   [0.6901961  0.45098042 0.20392159]\n",
      "   [0.7254902  0.48627454 0.2392157 ]]\n",
      "\n",
      "  [[0.5058824  0.2392157  0.08627451]\n",
      "   [0.5176471  0.2509804  0.09803922]\n",
      "   [0.5254902  0.25490198 0.09411766]\n",
      "   ...\n",
      "   [0.7058824  0.4666667  0.21176472]\n",
      "   [0.6901961  0.45098042 0.19607845]\n",
      "   [0.73333335 0.49411768 0.2392157 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.42352945 0.17254902 0.07450981]\n",
      "   [0.43921572 0.16470589 0.1254902 ]\n",
      "   [0.454902   0.16470589 0.12156864]\n",
      "   ...\n",
      "   [0.5137255  0.22352943 0.08627451]\n",
      "   [0.50980395 0.21568629 0.09019608]\n",
      "   [0.5058824  0.21176472 0.09019608]]\n",
      "\n",
      "  [[0.41960788 0.16862746 0.07058824]\n",
      "   [0.43137258 0.15686275 0.11764707]\n",
      "   [0.4431373  0.15294118 0.10980393]\n",
      "   ...\n",
      "   [0.5137255  0.22352943 0.08627451]\n",
      "   [0.50980395 0.21568629 0.09019608]\n",
      "   [0.5058824  0.21176472 0.09019608]]\n",
      "\n",
      "  [[0.4156863  0.16470589 0.06666667]\n",
      "   [0.427451   0.15294118 0.1137255 ]\n",
      "   [0.43529415 0.14509805 0.10196079]\n",
      "   ...\n",
      "   [0.5137255  0.22352943 0.08627451]\n",
      "   [0.50980395 0.21568629 0.09019608]\n",
      "   [0.5058824  0.21176472 0.09019608]]]\n",
      "\n",
      "\n",
      " [[[0.18039216 0.21568629 0.21176472]\n",
      "   [0.19215688 0.22352943 0.23529413]\n",
      "   [0.19607845 0.22352943 0.2627451 ]\n",
      "   ...\n",
      "   [0.29411766 0.38823533 0.5294118 ]\n",
      "   [0.2784314  0.35686275 0.49411768]\n",
      "   [0.2784314  0.3529412  0.50980395]]\n",
      "\n",
      "  [[0.16862746 0.20392159 0.23137257]\n",
      "   [0.16862746 0.20000002 0.24313727]\n",
      "   [0.15686275 0.18431373 0.25490198]\n",
      "   ...\n",
      "   [0.26666668 0.36078432 0.5019608 ]\n",
      "   [0.25882354 0.34901962 0.48235297]\n",
      "   [0.26666668 0.3529412  0.5058824 ]]\n",
      "\n",
      "  [[0.15686275 0.18823531 0.2627451 ]\n",
      "   [0.14901961 0.18039216 0.2627451 ]\n",
      "   [0.14901961 0.1764706  0.2784314 ]\n",
      "   ...\n",
      "   [0.26666668 0.36078432 0.50980395]\n",
      "   [0.2627451  0.34901962 0.49411768]\n",
      "   [0.25882354 0.3529412  0.5019608 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.16862746 0.22352943 0.3372549 ]\n",
      "   [0.17254902 0.227451   0.34117648]\n",
      "   [0.16862746 0.22352943 0.3372549 ]\n",
      "   ...\n",
      "   [0.20784315 0.30980393 0.5019608 ]\n",
      "   [0.21568629 0.31764707 0.50980395]\n",
      "   [0.21960786 0.32156864 0.52156866]]\n",
      "\n",
      "  [[0.18431373 0.227451   0.3372549 ]\n",
      "   [0.18431373 0.227451   0.3372549 ]\n",
      "   [0.17254902 0.21568629 0.3254902 ]\n",
      "   ...\n",
      "   [0.20000002 0.3019608  0.49411768]\n",
      "   [0.20784315 0.30980393 0.5019608 ]\n",
      "   [0.21568629 0.31764707 0.5176471 ]]\n",
      "\n",
      "  [[0.17254902 0.21176472 0.31764707]\n",
      "   [0.1764706  0.21568629 0.32156864]\n",
      "   [0.1764706  0.21568629 0.32156864]\n",
      "   ...\n",
      "   [0.20784315 0.30980393 0.5019608 ]\n",
      "   [0.21176472 0.3137255  0.5058824 ]\n",
      "   [0.20000002 0.3019608  0.5019608 ]]]\n",
      "\n",
      "\n",
      " [[[0.02745098 0.03137255 0.        ]\n",
      "   [0.04705883 0.0509804  0.01960784]\n",
      "   [0.04705883 0.0509804  0.01960784]\n",
      "   ...\n",
      "   [0.0627451  0.04313726 0.01960784]\n",
      "   [0.1137255  0.09411766 0.07058824]\n",
      "   [0.07058824 0.0509804  0.02745098]]\n",
      "\n",
      "  [[0.01960784 0.02352941 0.        ]\n",
      "   [0.03137255 0.03529412 0.00392157]\n",
      "   [0.03137255 0.03529412 0.00392157]\n",
      "   ...\n",
      "   [0.09411766 0.07450981 0.0509804 ]\n",
      "   [0.12156864 0.10196079 0.07843138]\n",
      "   [0.10588236 0.08627451 0.0627451 ]]\n",
      "\n",
      "  [[0.03529412 0.03921569 0.00784314]\n",
      "   [0.03921569 0.04313726 0.01176471]\n",
      "   [0.03529412 0.03921569 0.00784314]\n",
      "   ...\n",
      "   [0.09803922 0.07843138 0.05490196]\n",
      "   [0.07843138 0.05882353 0.03529412]\n",
      "   [0.1137255  0.09411766 0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.40000004 0.34509805 0.34509805]\n",
      "   [0.21960786 0.17254902 0.17254902]\n",
      "   [0.23529413 0.19607845 0.19215688]\n",
      "   ...\n",
      "   [0.24313727 0.18823531 0.15294118]\n",
      "   [0.5137255  0.45882356 0.4156863 ]\n",
      "   [0.54901963 0.48235297 0.4431373 ]]\n",
      "\n",
      "  [[0.20392159 0.18039216 0.18823531]\n",
      "   [0.23137257 0.21176472 0.20000002]\n",
      "   [0.30980393 0.2901961  0.26666668]\n",
      "   ...\n",
      "   [0.5686275  0.5019608  0.47450984]\n",
      "   [0.53333336 0.45882356 0.43137258]\n",
      "   [0.5372549  0.4666667  0.427451  ]]\n",
      "\n",
      "  [[0.39607847 0.37254903 0.3803922 ]\n",
      "   [0.3803922  0.36078432 0.34901962]\n",
      "   [0.4156863  0.39607847 0.37254903]\n",
      "   ...\n",
      "   [0.31764707 0.2509804  0.22352943]\n",
      "   [0.5058824  0.43137258 0.4039216 ]\n",
      "   [0.5254902  0.454902   0.4156863 ]]]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-efa04591b404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-efa04591b404>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(directory, sample_count)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfeatures_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Takanori/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        # print(inputs_batch)\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512)) # 2000行と4*4*512の特徴量に変換\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5913 - acc: 0.6810 - val_loss: 0.4313 - val_acc: 0.8180\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4155 - acc: 0.8070 - val_loss: 0.3476 - val_acc: 0.8720\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3456 - acc: 0.8510 - val_loss: 0.3109 - val_acc: 0.8850\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3156 - acc: 0.8695 - val_loss: 0.2926 - val_acc: 0.8920\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2764 - acc: 0.8775 - val_loss: 0.2801 - val_acc: 0.8860\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2636 - acc: 0.8920 - val_loss: 0.2699 - val_acc: 0.8950\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2517 - acc: 0.9030 - val_loss: 0.2656 - val_acc: 0.8920\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2303 - acc: 0.9135 - val_loss: 0.2541 - val_acc: 0.8950\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2181 - acc: 0.9135 - val_loss: 0.2492 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2002 - acc: 0.9175 - val_loss: 0.2495 - val_acc: 0.8940\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1864 - acc: 0.9325 - val_loss: 0.2516 - val_acc: 0.8930\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1869 - acc: 0.9305 - val_loss: 0.2383 - val_acc: 0.9020\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1767 - acc: 0.9365 - val_loss: 0.2365 - val_acc: 0.9050\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1681 - acc: 0.9415 - val_loss: 0.2410 - val_acc: 0.9030\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1583 - acc: 0.9410 - val_loss: 0.2335 - val_acc: 0.9070\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1562 - acc: 0.9470 - val_loss: 0.2346 - val_acc: 0.9080\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1477 - acc: 0.9475 - val_loss: 0.2306 - val_acc: 0.9040\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1423 - acc: 0.9495 - val_loss: 0.2370 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1301 - acc: 0.9555 - val_loss: 0.2359 - val_acc: 0.8990\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1287 - acc: 0.9570 - val_loss: 0.2301 - val_acc: 0.9060\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1233 - acc: 0.9590 - val_loss: 0.2303 - val_acc: 0.9090\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1197 - acc: 0.9615 - val_loss: 0.2520 - val_acc: 0.8930\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1178 - acc: 0.9635 - val_loss: 0.2305 - val_acc: 0.9100\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1116 - acc: 0.9630 - val_loss: 0.2319 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1058 - acc: 0.9670 - val_loss: 0.2310 - val_acc: 0.9040\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1056 - acc: 0.9630 - val_loss: 0.2313 - val_acc: 0.9020\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0973 - acc: 0.9695 - val_loss: 0.2396 - val_acc: 0.9060\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0944 - acc: 0.9715 - val_loss: 0.2361 - val_acc: 0.9010\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0890 - acc: 0.9735 - val_loss: 0.2329 - val_acc: 0.9020\n",
      "Epoch 30/30\n",
      "1060/2000 [==============>...............] - ETA: 1s - loss: 0.0741 - acc: 0.9774"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
